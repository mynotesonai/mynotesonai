{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af289d0c",
   "metadata": {},
   "source": [
    "# Finetuning HuggingFace Vision Transformer with Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674aac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from seaborn) (1.23.2)\n",
      "Collecting matplotlib!=3.6.1,>=3.1\n",
      "  Downloading matplotlib-3.6.2-cp310-cp310-macosx_11_0_arm64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from seaborn) (1.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pamelagangopadhyay/miniforge3/envs/wave2vec2_ft/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: matplotlib, seaborn\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.6.1\n",
      "    Uninstalling matplotlib-3.6.1:\n",
      "      Successfully uninstalled matplotlib-3.6.1\n",
      "Successfully installed matplotlib-3.6.2 seaborn-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13f0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1aebf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72fe97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ef4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9387a2e",
   "metadata": {},
   "source": [
    "### Loading data and creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db5042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((150,150)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder('data/plastic/train', transform=transform)\n",
    "test_ds = torchvision.datasets.ImageFolder('data/plastic/test', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8769adf",
   "metadata": {},
   "source": [
    "### Finetuning pretrained vit-base-patch16-224-in21k model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d62604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "            return logits, loss.item()\n",
    "        else:\n",
    "            return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34d3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839e6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['bottles', 'cans', 'sprays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f015f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define Model\n",
    "model = ViTForImageClassification(len(train_ds.classes))\n",
    "\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80231425",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2a3ca",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9d8dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  780\n",
      "Number of test samples:  30\n",
      "Detected Classes are:  {'bottles': 0, 'cans': 1, 'sprays': 2}\n",
      "Epoch:  0 | train loss: 1.1196\n",
      "Epoch:  0 | train loss: 0.4546\n",
      "Epoch:  0 | train loss: 0.1524\n",
      "Epoch:  0 | train loss: 0.0690\n",
      "Epoch:  0 | train loss: 1.1545\n",
      "Epoch:  0 | train loss: 0.1089\n",
      "Epoch:  0 | train loss: 0.1731\n",
      "Epoch:  0 | train loss: 0.4896\n",
      "Epoch:  1 | train loss: 0.0474\n",
      "Epoch:  1 | train loss: 0.0419\n",
      "Epoch:  1 | train loss: 0.0607\n",
      "Epoch:  1 | train loss: 0.0420\n",
      "Epoch:  1 | train loss: 0.3447\n",
      "Epoch:  1 | train loss: 0.1624\n",
      "Epoch:  1 | train loss: 0.0372\n",
      "Epoch:  1 | train loss: 0.0655\n",
      "Epoch:  2 | train loss: 0.6340\n",
      "Epoch:  2 | train loss: 0.7879\n",
      "Epoch:  2 | train loss: 0.8784\n",
      "Epoch:  2 | train loss: 0.5107\n",
      "Epoch:  2 | train loss: 1.1252\n",
      "Epoch:  2 | train loss: 0.6410\n",
      "Epoch:  2 | train loss: 0.9133\n",
      "Epoch:  2 | train loss: 0.2076\n",
      "Epoch:  3 | train loss: 0.0432\n",
      "Epoch:  3 | train loss: 0.0267\n",
      "Epoch:  3 | train loss: 0.1507\n",
      "Epoch:  3 | train loss: 0.2346\n",
      "Epoch:  3 | train loss: 0.0470\n",
      "Epoch:  3 | train loss: 0.0235\n",
      "Epoch:  3 | train loss: 0.0195\n",
      "Epoch:  3 | train loss: 0.0153\n",
      "Epoch:  4 | train loss: 0.0181\n",
      "Epoch:  4 | train loss: 0.0462\n",
      "Epoch:  4 | train loss: 0.0200\n",
      "Epoch:  4 | train loss: 0.0166\n",
      "Epoch:  4 | train loss: 0.3125\n",
      "Epoch:  4 | train loss: 0.0244\n",
      "Epoch:  4 | train loss: 0.0167\n",
      "Epoch:  4 | train loss: 0.0140\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))\n",
    "print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "        for index, array in enumerate(x):\n",
    "            x[index] = np.squeeze(array)\n",
    "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "        x, y  = x.to(device), y.to(device)\n",
    "        b_x = Variable(x)\n",
    "        b_y = Variable(y)\n",
    "        output, loss = model(b_x, None)\n",
    "        # Calculate loss\n",
    "        if loss is None: \n",
    "            loss = loss_func(output, b_y)   \n",
    "            optimizer.zero_grad()           \n",
    "            loss.backward()                 \n",
    "            optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ed3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/plastic/models/vt_finetuned_v1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb74c2",
   "metadata": {},
   "source": [
    "### Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404a72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_1 = torchvision.datasets.ImageFolder('data/plastic/test_images', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f948d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_1  = data.DataLoader(test_ds_1, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69a8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 0, 2, 1, 1, 2, 0, 1, 2])\n",
      "tensor([0, 1, 0, 2, 1, 0, 2, 0, 1, 2])\n",
      " test accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "test_1 = next(iter(test_loader_1))\n",
    "test_1_x = test_1[0]\n",
    "test_1_x = np.split(np.squeeze(np.array(test_1_x)), BATCH_SIZE)\n",
    "for index, array in enumerate(test_1_x):\n",
    "    test_1_x[index] = np.squeeze(array)\n",
    "\n",
    "test_1_x = torch.tensor(np.stack(feature_extractor(test_1_x)['pixel_values'], axis=0))\n",
    "\n",
    "test_1_x = test_1_x.to(device)\n",
    "test_1_y = test_1[1].to(device)\n",
    "\n",
    "test_output_1, loss_1 = model(test_1_x, test_1_y)\n",
    "test_output_1 = test_output_1.argmax(1)\n",
    "print(test_output_1)\n",
    "print(test_1_y)\n",
    "\n",
    "accuracy_ = (test_output_1 == test_1_y).sum().item() / BATCH_SIZE\n",
    "print(' test accuracy: %.2f' % accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d4cbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recognition_accuracy(predicted, actual):\n",
    "    num_correct_predictions = np.sum(predicted == actual)\n",
    "    recognition_acc = num_correct_predictions/len(actual)\n",
    "    return recognition_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a9f19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_acc = calculate_recognition_accuracy(np.array(test_output_1), np.array(test_1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc25b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recognition accuracy for the test images 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f'The recognition accuracy for the test images {recog_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3f2ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_1_y, test_output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6b003b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4, 1: 3, 2: 3})\n",
      "Accuracy of bottles 75.0\n",
      "Accuracy of cans 66.66666666666667\n",
      "Accuracy of sprays 100.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_labels = np.array(test_1_y)\n",
    "act_dict = Counter(list(test_labels))\n",
    "pred_list = cm.diagonal()\n",
    "        \n",
    "for i in range(len(labels)):\n",
    "    label_recog = 100 * pred_list[i] / act_dict[i]\n",
    "    print('Accuracy of', labels[i], label_recog)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "870802a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXwUlEQVR4nO3de3RV5bnv8e8TLpVbAAcFAgmgAS/YA0J3VerYu0CpFY677HqsraNHKlqxFxTPFq2H9tj29GKt3e6D4pYdK6063GhKqTIs5bRDobRuqdxvxUsSCgkE8UITI0GFPuePLD2ZK5e1VhIy58v6fRxzkDXnXO96xpT5433fOeeKuTsiIiEpiLsAEZFcKbhEJDgKLhEJjoJLRIKj4BKR4Ci4RCQ4Ci4ROWnM7DQze8HMtpvZbjP7biv7mJnda2YVZrbDzCZnarfnySlXRASAd4Dp7t5gZr2AP5rZb9x9Q7N9ZgLjUsuFwAOpP9ukHpeInDTepCH1sldqSb/rfTbwSGrfDcAgMytqr92T3uPqM2m+bs3P0qM//2bcJQThsvPa/TstKaf1xDryvlzO2WPb7r8BmNdsVZm7lzXfx8x6AJuBscD97v6ntGZGAtXNXtek1tW29bkaKopIlGU/EEuFVFmGfU4A55vZIOBXZvYRd9/V/BNbe1t7bWqoKCJRZtkvOXD3vwLrgEvTNtUAJc1eFwMH22tLwSUiUVaQ/ZKpKbMPp3pamFkfYAbwYtpuq4A5qauLFwF17t7mMBE0VBSRdDn2pDIoAh5OzXMVAOXu/rSZfQXA3ZcCq4FZQAVwFJibqVEFl4hEFfTosqbcfQcwqZX1S5v97MDXc2lXwSUiUTlMzsdFwSUiUV07VDwpFFwiEqUel4gERz0uEQmOelwiEpwuvKp4sii4RCRKPS4RCU6B5rhEJDTqcYlIcHRVUUSCo8l5EQmOhooiEhwNFUUkOOpxiUhw1OMSkeCoxyUiwdFVRREJjnpcIhIczXGJSHDU4xKR4KjHJSLBUY9LREJjBQouEQmMaaiYHDOmnMst18zgnDOLGFzYh9ePNLBh+16+/++rebHqUNzlJUbdG4dZ/9RyDlS+xKF9lbz37jssXLKcwUOL4i4tcQ7V1nL3XXey4fnncHcunPJxbvvGIopGjIi7tM5Jfm6R/D5hFzl9YF+27qnmn+8q57Kv3c//um8V55YW8fuHb2FU0eC4y0uMNw4dYOfz6+jTfwCjz50QdzmJ1djYyPXXfom9e6v43g/v4gc/+jH79+3jy9fO4ejRo3GX1ylmlvUSl7zpcZWv2Uz5ms2RdZt27WPHk3fw2RmTWPzoszFVlixjzp3Iogd/BcDGZ56mYvvGmCtKppUryqmpqeapp9cwavRoAMaddTafmfVpVpQ/wZxr5sZcYceFMFTMmx5Xa96sexuA946fiLmS5CgIYGI2CdatfZYJEyZ+EFoAxcUlnD9pMuvWPhNjZZ1XUFCQ9ZKJmZWY2Voz22Nmu81sQSv7TDWzOjPbllruyNRu3vS43ldQYPQoKGDUiNP5/k2zqX2tjl+k9cREMqmsqGDq9E+2WF9aOpbf/XZNDBV1oa7tcB0HbnH3LWY2ANhsZr9z9z+n7fcHd78s20bzLrjWP3orHx0/CoCK/YeZecO9vHakIeaqJDR1dXUUFha2WD9w4EDq6+tjqKjrdOVQ0d1rgdrUz2+Z2R5gJJAeXDnJGFxmdg4wO/VhDhwEVrn7ns58cFyu+9bDFPY7jTOKh7Dg6k/y6wfmM33uv7K/9s24S5PAtHaCewx1dLWTNcdlZmOAScCfWtk8xcy205QvC919d3tttTtINbNvAI/T1Hl8AdiY+nm5md3ezvvmmdkmM9t0/PV2P7/bvbT3VTbu2kf5ms3MuuE++vX9EAuvvSTusiQwhQMLqaura7G+vo2eWEhyuarY/FxPLfPaaLM/8EvgZndP75JuAUa7+0TgPuDJTDVm6nFdB5zn7u+lFXEPsBv4UWtvcvcyoAygz6T5if1HqK6hkcrq1yktGRJ3KRKY0tKxVFa80mJ9VVUlZ5aOjaGirpNLj6v5ud5Oe71oCq3H3H1lK23UN/t5tZn9m5kNcffX22oz02WBvwGt3U1XlNoWtKGnD+DsMcOoqm7z+Ii0auq06ezcsZ2a6uoP1h04UMO2rVv4xLTpMVbWeVZgWS8Z22pKwYeAPe5+Txv7DE/th5ldQFMuvdFeu5l6XDcDz5jZK8D7/4dGAWOB+RmrTpAn/uV6tr5Yza5XDlDfcIxxo4dy4xencfzECd3DlWbXhnUAHKx6GYCXt71Av8KB9CscxBnjz4+vsAS5/Iorefw/HmPBjV9j/k0LMIz771vMsOHD+dznPh93eZ3SxXNcFwNXAzvNbFtq3SKacgR3XwpcAXzVzI4DjcAX3L3dkZpl2I6ZFQAX0DQ5b0ANsNHds7r5KSlDxVuumcHln5rMmSVD6N2zJzWvHmH9ple4e9lvEzMx/+jPvxl3CQB888qpra4/Y/xEvvydxd1bTCsuOy8Zjx/VHjwYfeTnoincevsiRo4sjrs0AE7r2bEbG4ZeW571OXt42ZWx3K2aMbg6KynBFYKkBFfSJSW4kq7DwXVdDsH1UDzBlXf3cYlI+0J45EfBJSIRCi4RCU4Iz6squEQkKvkdLgWXiERpqCgiwVFwiUhwFFwiEpxsHuWJm4JLRCLU4xKR4Ci4RCQ4Ci4RCU/yc0vBJSJR6nGJSHAKdFVRREKjHpeIBCeA3FJwiUiUelwiEpwAckvBJSJRmpwXkeAouEQkOBoqikhwNDkvIsFRcIlIcALILQWXiERpcl5EgqOhoogEJ4DcIvm/+VFEupWZZb1k0VaJma01sz1mttvMFrSyj5nZvWZWYWY7zGxypnbV4xKRiC7ucR0HbnH3LWY2ANhsZr9z9z8322cmMC61XAg8kPqzTepxiUhEV/a43L3W3bekfn4L2AOMTNttNvCIN9kADDKzovbaPek9riMbl5zsjzhlTP3J7+MuQU4hV0xs99xvUy5XFc1sHjCv2aoydy9rY98xwCTgT2mbRgLVzV7XpNbVtvW5GiqKSEQuQ8VUSLUaVNE2rT/wS+Bmd69P39xa0+21p+ASkYiuvh3CzHrRFFqPufvKVnapAUqavS4GDrbXpua4RCTCLPslc1tmwEPAHne/p43dVgFzUlcXLwLq3L3NYSKoxyUiabq4x3UxcDWw08y2pdYtAkYBuPtSYDUwC6gAjgJzMzWq4BKRiK4MLnf/Ixl+U6O7O/D1XNpVcIlIhJ5VFJHghPDIj4JLRCL0kLWIBCeA3FJwiUhUQQDJpeASkQhNzotIcALILQWXiERpcl5EghNAbim4RCTK2r/RPREUXCISoTkuEQmOriqKSHB0H5eIBCeA3FJwiUiUbocQkeAEkFsKLhGJ6hFAcim4RCRCQ0URCU4Ad0MouEQkSj0uEQlOALml4BKRKPW4RCQ4PQKY5Mqb4DpUW8vdd93Jhuefw925cMrHue0biygaMSLu0hJl2tlDuGT8UM4ZPoDBfXvxav07rHv5dR5+fj9H3z0Rd3mJUvfGYdY/tZwDlS9xaF8l7737DguXLGfw0KK4S+uU5McWFMRdQHdobGzk+mu/xN69VXzvh3fxgx/9mP379vHla+dw9OjRuMtLlC9eUMKJvzlLf7+X/1G+k5VbD3L5pBHc+/kJQfyF7k5vHDrAzufX0af/AEafOyHucrpMgVnWS1zyose1ckU5NTXVPPX0GkaNHg3AuLPO5jOzPs2K8ieYc03G3/idNxau2MVfG9/74PXW6jrqjx3n25edw+TRg9i876/xFZcwY86dyKIHfwXAxmeepmL7xpgr6hoBTHHlR49r3dpnmTBh4gehBVBcXML5kyazbu0zMVaWPM1D6317at8C4MP9e3d3OYlWUHBqnj5mlvUSl1PzyKeprKigdNxZLdaXlo6lqrIihorCMqlkIAB/eUPD6nxglv2SuS1bZmaHzWxXG9unmlmdmW1LLXdkU2NeDBXr6uooLCxssX7gwIHU19fHUFE4Pty/N9f//Rhe2HuEFw81xF2OdIMuvqr4c2AJ8Eg7+/zB3S/LpdG8CC5o/d4Uj6GOkPTpVcCP/9tHOPE353urX4y7HOkmXTkEdPf1ZjamyxpM6fBQ0czanNE2s3lmtsnMNj30YFlHP6LLFA4spK6ursX6+jZ6YgK9exh3X/ERRgw6jZvLd/LaW+/GXZJ0k4IclubnemqZ14GPnGJm283sN2Z2XjZv6EyP67vAz1rb4O5lQBnAsePxd2xKS8dSWfFKi/VVVZWcWTo2hoqSrUeBcednz2N8USE3Pr6dytfejrsk6Ua59Lian+sdtAUY7e4NZjYLeBIYl+lN7fa4zGxHG8tOYFgniu1WU6dNZ+eO7dRUV3+w7sCBGrZt3cInpk2PsbLkMeC7/3gOfzdmMLf9che7D74Vd0nSzQos+6Wz3L3e3RtSP68GepnZkEzvy9TjGgZ8GjiStt6A/+xIoXG4/Iorefw/HmPBjV9j/k0LMIz771vMsOHD+dznPh93eYly6yXjmHHuUH723D4a3zvBeSMGfLDt8FvvaMiYZteGdQAcrHoZgJe3vUC/woH0KxzEGePPj6+wTujOR37MbDjwqru7mV1AU2fqjUzvyxRcTwP93X1bKx+4rgN1xqJv3748uOxh7r7rTr55+21Nj/xcNIVbb19E33794i4vUaaUng7A3ItHM/fi0ZFtP/3jX/jpH/fFUVZiLb/nO5HXq376rwCcMX4iX/7O4hgq6ryuzC0zWw5MBYaYWQ3wbaAXgLsvBa4Avmpmx4FG4AvunnF6ybLYp1OSMMcViqk/+X3cJQRh4cyW9+RJS1dMLOpQBN3265eyPmd//F/PjuUu1Ly5HUJEsqPfqygiwQnhcRoFl4hEBNDhUnCJSJS+SFBEghNAbim4RCRKk/MiEpwAckvBJSJRGiqKSHAsgN8uoOASkYieAdzIpeASkQj9QlgRCY7muEQkOAF0uBRcIhKl+7hEJDg9NDkvIqEp0O0QIhKaAEaKCi4RidJVRREJjibnRSQ4AeSWgktEovRFgiISnADuhlBwiUiUnlUUkeAkP7YUXCKSRlcVRSQ4yY8tBZeIpCkI4KpiCBcQRKQbFeSwZGJmy8zssJntamO7mdm9ZlZhZjvMbHK2NYqIfMDMsl6y8HPg0na2zwTGpZZ5wAPZNKrgEpEIy2HJxN3XA2+2s8ts4BFvsgEYZGZFmdrVHFeCrFv4ibhLCMLgj82Pu4QgXLF1SYfel8t9XGY2j6ae0vvK3L0sh48bCVQ3e12TWlfb3psUXCIS0SOH4EqFVC5Bla61D/NMb1JwiUhEN19TrAFKmr0uBg5mepPmuEQkwiz7pQusAuakri5eBNS5e7vDRFCPS0TSdOVXN5vZcmAqMMTMaoBvA70A3H0psBqYBVQAR4G52bSr4BKRiK584sfdr8qw3YGv59qugktEIiyAh34UXCISkctVxbgouEQkIoDcUnCJSJSCS0SCozkuEQlOAN9qo+ASkSh9A6qIBEdDRREJjoaKIhIc9bhEJDgBTHEpuEQkKoDcUnCJSJQe+RGR8CQ/txRcIhKlyXkRCU4AI0UFl4hEBZBbCi4RSRNAcim4RCRCzyqKSHCSH1sKLhFJF0ByKbhEJEK3Q4hIcAKY4lJwiUiUgitBDtXWcvddd7Lh+edwdy6c8nFu+8YiikaMiLu0xNGxymzGlHO55ZoZnHNmEYML+/D6kQY2bN/L9/99NS9WHYq7vE4JYahoTb9I9uQ5dpyT+wFZaGxs5MrLZ9Ord2/m33QzZrDk3sUcO9bIL1auom/fvnGXmBghHKvBH5sfdwlceelHOf+cEjbu+guvHWmgZPhgFs69hOJhg/jYlT9kf+2RuEukceuSDiXQzpqGrM/Z/1LcP5aUy4se18oV5dTUVPPU02sYNXo0AOPOOpvPzPo0K8qfYM41c2OuMDl0rLJTvmYz5Ws2R9Zt2rWPHU/ewWdnTGLxo8/GVFnnJb+/BQVxF9Ad1q19lgkTJn5wIgIUF5dw/qTJrFv7TIyVJY+OVce9Wfc2AO8dPxFzJZ1kOSzZNGd2qZm9ZGYVZnZ7K9unmlmdmW1LLXdkajMvgquyooLScWe1WF9aOpaqyooYKkouHavcFBQYvXr2oHTUh1nyrauofa2OX6T1xEJjOfyXsS2zHsD9wExgPHCVmY1vZdc/uPv5qeV/Z2o3L4aKdXV1FBYWtlg/cOBA6uvrY6gouXSscrP+0Vv56PhRAFTsP8zMG+7ltSMNMVfVOV38yzIuACrcvQrAzB4HZgN/7kyjGXtcZnaOmX3SzPqnrb+0Mx/c3ayVa7yxXzVIKB2r7F33rYf5h6vv5kv/82fUNxzj1w/MZ1TR6XGX1TldO1QcCVQ3e12TWpduipltN7PfmNl5mRptN7jM7CbgKeBGYJeZzW62+YftvG+emW0ys00PPViWqYaTrnBgIXV1dS3W17fRu8hnOla5eWnvq2zctY/yNZuZdcN99Ov7IRZee0ncZXVKLkPF5ud6apnXormW0v8d3AKMdveJwH3Ak5lqzDRUvB74qLs3mNkYYIWZjXH3xW0U1FSVexlQBsm4HaK0dCyVFa+0WF9VVcmZpWNjqCi5dKw6rq6hkcrq1yktGRJ3KZ2Syw2ozc/1NtQAJc1eFwMH09qob/bzajP7NzMb4u6vt9VopqFiD3dvSDX4F2AqMNPM7iGMq6YATJ02nZ07tlNT/f97rAcO1LBt6xY+MW16jJUlj45Vxw09fQBnjxlGVXWb51sQuvii4kZgnJmdYWa9gS8AqyKfZzbcUvMTZnYBTbn0Rrs1tncDqpk9C/yzu29rtq4nsAz4orv3yFR1EnpcR48e5crLZ/Oh005j/k0LMIz771vM20ffZsXKVfTt1y/uEhMjhGOVhBtQn/iX69n6YjW7XjlAfcMxxo0eyo1fnMawIYX8/X//CRX7D8ddYodvQH351aNZn7NnDeub8TPMbBbwf4AewDJ3/4GZfQXA3Zea2Xzgq8BxoJGmzPnPdtvMEFzFwHF3b/EMg5ld7O7PZSo6CcEFUHvwYPQxloumcOvtixg5sjju0hIn6ccqCcF1yzUzuPxTkzmzZAi9e/ak5tUjrN/0Cncv+y37a9+Muzyg48FVcbgx63N27NA+sYy88uKRHzm1JCG4QtDR4KrMIbhKYwquvLiPS0RyEMDstYJLRCJC+HYIBZeIROj7uEQkOAouEQmOhooiEhz1uEQkOAHkloJLRKLU4xKRACU/uRRcIhLRxV8keFIouEQkQkNFEQmObocQkfAkP7cUXCISFUBuKbhEJEpzXCISnNZ+y1PSKLhEJCL5saXgEpE0AXS4FFwiEqXbIUQkOOpxiUhwFFwiEhwNFUUkOOpxiUhwAsgtBZeIpAkguRRcIhKhOS4RCU4IXyRYEHcBIpIwlsOSTXNml5rZS2ZWYWa3t7LdzOze1PYdZjY5U5sKLhGJsBz+y9iWWQ/gfmAmMB64yszGp+02ExiXWuYBD2RqV8ElIhFm2S9ZuACocPcqd38XeByYnbbPbOARb7IBGGRmRe01etLnuE7rmbyZPjOb5+5lcdcRgiQeq8atS+IuoYUkHqeOyuWcNbN5NPWS3leWdhxGAtXNXtcAF6Y109o+I4Hatj43X3tc8zLvIik6VtnJy+Pk7mXu/nfNlvTwbi0EvQP7RORrcIlI96gBSpq9LgYOdmCfCAWXiJxMG4FxZnaGmfUGvgCsSttnFTAndXXxIqDO3dscJkL+3sd1SsxFdBMdq+zoOLXC3Y+b2Xzg/wI9gGXuvtvMvpLavhRYDcwCKoCjwNxM7Zp7u0NJEZHE0VBRRIKj4BKR4ORdcGV6/ECamNkyMztsZrviriXJzKzEzNaa2R4z221mC+KuKR/k1RxX6vGDl4FP0XQJdiNwlbv/OdbCEsjM/gFooOmO5o/EXU9Spe7wLnL3LWY2ANgM/JP+Tp1c+dbjyubxAwHcfT3wZtx1JJ2717r7ltTPbwF7aLrrW06ifAuuth4tEOk0MxsDTAL+FHMpp7x8C66cHy0QyYaZ9Qd+Cdzs7vVx13Oqy7fgyvnRApFMzKwXTaH1mLuvjLuefJBvwZXN4wciWTMzAx4C9rj7PXHXky/yKrjc/Tjw/uMHe4Byd98db1XJZGbLgeeBs82sxsyui7umhLoYuBqYbmbbUsusuIs61eXV7RAicmrIqx6XiJwaFFwiEhwFl4gER8ElIsFRcIlIcBRcIhIcBZeIBOf/AXTdxlHaw5A5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (5,4))\n",
    "sn.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3c916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
